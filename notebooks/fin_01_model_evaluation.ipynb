{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58370e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running model GradientBoosting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRFClassifier\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Run all models\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m rows \u001b[38;5;241m=\u001b[39m benchmark_classifiers(X, y, [\n\u001b[1;32m    157\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradientBoosting\u001b[39m\u001b[38;5;124m\"\u001b[39m, GradientBoostingClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)),\n\u001b[1;32m    158\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistGradientBoosting\u001b[39m\u001b[38;5;124m\"\u001b[39m, HistGradientBoostingClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)),\n\u001b[1;32m    159\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecisionTree\u001b[39m\u001b[38;5;124m\"\u001b[39m, DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)),\n\u001b[1;32m    160\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdaBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m, AdaBoostClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)),\n\u001b[1;32m    161\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraTrees\u001b[39m\u001b[38;5;124m\"\u001b[39m, ExtraTreesClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)),\n\u001b[1;32m    162\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m\"\u001b[39m, RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)),\n\u001b[1;32m    163\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgbDART\u001b[39m\u001b[38;5;124m\"\u001b[39m, XGBClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state, booster\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m    164\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgbTree\u001b[39m\u001b[38;5;124m\"\u001b[39m, XGBClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)),\n\u001b[1;32m    165\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDummy\u001b[39m\u001b[38;5;124m\"\u001b[39m, DummyClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m    166\u001b[0m ])\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Print results table\u001b[39;00m\n\u001b[1;32m    169\u001b[0m table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows, columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFN (secret)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    189\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[3], line 107\u001b[0m, in \u001b[0;36mbenchmark_classifiers\u001b[0;34m(X, y, competitors)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m competitor \u001b[38;5;129;01min\u001b[39;00m competitors:\n\u001b[1;32m    106\u001b[0m         name, estimator \u001b[38;5;241m=\u001b[39m competitor\n\u001b[0;32m--> 107\u001b[0m         name, results, f1_secret, roc_auc_secret, secret_tp, secret_fp, secret_tn, secret_fn \u001b[38;5;241m=\u001b[39m benchmark_classifier(name, estimator, X, y, X2, y2)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m#         future = executor.submit(benchmark_classifier, name, estimator, X, y, X2, y2)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#         futures_.append(future)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#     # Iterate over the completed futures to get the results\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m#     for future in concurrent.futures.as_completed(futures_):\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#         name, results, f1_secret = future.result()\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([\n\u001b[1;32m    117\u001b[0m             name,\n\u001b[1;32m    118\u001b[0m             np\u001b[38;5;241m.\u001b[39mmax(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m             np\u001b[38;5;241m.\u001b[39mmean(secret_fn),\n\u001b[1;32m    140\u001b[0m         ])\n",
      "Cell \u001b[0;32mIn[3], line 62\u001b[0m, in \u001b[0;36mbenchmark_classifier\u001b[0;34m(name, estimator, X, y, X2, y2)\u001b[0m\n\u001b[1;32m     49\u001b[0m scoring \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m }\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Running model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m results \u001b[38;5;241m=\u001b[39m cross_validate(estimator, X, y, cv\u001b[38;5;241m=\u001b[39mStratifiedKFold(shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m     63\u001b[0m duration \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:668\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[1;32m    669\u001b[0m     X,\n\u001b[1;32m    670\u001b[0m     y,\n\u001b[1;32m    671\u001b[0m     raw_predictions,\n\u001b[1;32m    672\u001b[0m     sample_weight,\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[1;32m    674\u001b[0m     X_val,\n\u001b[1;32m    675\u001b[0m     y_val,\n\u001b[1;32m    676\u001b[0m     sample_weight_val,\n\u001b[1;32m    677\u001b[0m     begin_at_stage,\n\u001b[1;32m    678\u001b[0m     monitor,\n\u001b[1;32m    679\u001b[0m )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:745\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    738\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    739\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    740\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    741\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    742\u001b[0m     )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[1;32m    746\u001b[0m     i,\n\u001b[1;32m    747\u001b[0m     X,\n\u001b[1;32m    748\u001b[0m     y,\n\u001b[1;32m    749\u001b[0m     raw_predictions,\n\u001b[1;32m    750\u001b[0m     sample_weight,\n\u001b[1;32m    751\u001b[0m     sample_mask,\n\u001b[1;32m    752\u001b[0m     random_state,\n\u001b[1;32m    753\u001b[0m     X_csc,\n\u001b[1;32m    754\u001b[0m     X_csr,\n\u001b[1;32m    755\u001b[0m )\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:250\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    247\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, residual, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    251\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    252\u001b[0m     X,\n\u001b[1;32m    253\u001b[0m     y,\n\u001b[1;32m    254\u001b[0m     residual,\n\u001b[1;32m    255\u001b[0m     raw_predictions,\n\u001b[1;32m    256\u001b[0m     sample_weight,\n\u001b[1;32m    257\u001b[0m     sample_mask,\n\u001b[1;32m    258\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m    259\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# add tree to ensemble\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[i, k] \u001b[38;5;241m=\u001b[39m tree\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb_losses.py:121\u001b[0m, in \u001b[0;36mLossFunction.update_terminal_regions\u001b[0;34m(self, tree, X, y, residual, raw_predictions, sample_weight, sample_mask, learning_rate, k)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# update each leaf (= perform line search)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m leaf \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(tree\u001b[38;5;241m.\u001b[39mchildren_left \u001b[38;5;241m==\u001b[39m TREE_LEAF)[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_terminal_region(\n\u001b[1;32m    122\u001b[0m         tree,\n\u001b[1;32m    123\u001b[0m         masked_terminal_regions,\n\u001b[1;32m    124\u001b[0m         leaf,\n\u001b[1;32m    125\u001b[0m         X,\n\u001b[1;32m    126\u001b[0m         y,\n\u001b[1;32m    127\u001b[0m         residual,\n\u001b[1;32m    128\u001b[0m         raw_predictions[:, k],\n\u001b[1;32m    129\u001b[0m         sample_weight,\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# update predictions (both in-bag and out-of-bag)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m raw_predictions[:, k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m tree\u001b[38;5;241m.\u001b[39mvalue[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtake(\n\u001b[1;32m    134\u001b[0m     terminal_regions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    135\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb_losses.py:730\u001b[0m, in \u001b[0;36mBinomialDeviance._update_terminal_region\u001b[0;34m(self, tree, terminal_regions, leaf, X, y, residual, raw_predictions, sample_weight)\u001b[0m\n\u001b[1;32m    727\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39mtake(terminal_region, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    729\u001b[0m numerator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(sample_weight \u001b[38;5;241m*\u001b[39m residual)\n\u001b[0;32m--> 730\u001b[0m denominator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(sample_weight \u001b[38;5;241m*\u001b[39m (y \u001b[38;5;241m-\u001b[39m residual) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y \u001b[38;5;241m+\u001b[39m residual))\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# prevents overflow and division by zero\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(denominator) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-150\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import clone as clone_estimator\n",
    "import tlib\n",
    "import concurrent.futures\n",
    "\n",
    "# Pseudo random state\n",
    "random_state = 42\n",
    "\n",
    "# Load data\n",
    "X, y = tlib.load_tfw1()\n",
    "X2, y2 = tlib.load_tfw2()\n",
    "\n",
    "executor = concurrent.futures.ProcessPoolExecutor()\n",
    "\n",
    "def predict_secret(estimator, X, y, X2, y2, i):\n",
    "    from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "    \n",
    "    rs = random_state + i if isinstance(random_state, int) else random_state\n",
    "    X_shuffled, y_shuffled = shuffle(X, y, random_state=rs)\n",
    "    clone = clone_estimator(estimator)\n",
    "    clone.fit(X_shuffled, y_shuffled)\n",
    "    y_pred = clone.predict(X2)\n",
    "    y_proba = clone.predict_proba(X2)[:, 1]\n",
    "    \n",
    "    cm = confusion_matrix(y2, y_pred)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fn = cm[1, 0]\n",
    "    \n",
    "    return f1_score(y2, y_pred), roc_auc_score(y2, y_proba), tp, fp, tn, fn\n",
    "\n",
    "def benchmark_classifier(name, estimator, X, y, X2, y2):\n",
    "    from sklearn.model_selection import cross_val_score, cross_validate\n",
    "    from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "    from sklearn.metrics import make_scorer, confusion_matrix, f1_score, precision_score\n",
    "    \n",
    "    def tn_scorer(y_true, y_pred):\n",
    "        return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "    def fp_scorer(y_true, y_pred):\n",
    "        return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "    def fn_scorer(y_true, y_pred):\n",
    "        return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "    def tp_scorer(y_true, y_pred):\n",
    "        return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "\n",
    "    scoring = {\n",
    "        \"f1\": \"f1\",\n",
    "        \"accuracy\": \"accuracy\",\n",
    "        \"precision\": make_scorer(precision_score, zero_division=1),\n",
    "        \"recall\": \"recall\",\n",
    "        \"tn\": make_scorer(tn_scorer),\n",
    "        \"fp\": make_scorer(fp_scorer),\n",
    "        \"fn\": make_scorer(fn_scorer),\n",
    "        \"tp\": make_scorer(tp_scorer),\n",
    "        \"roc_auc\": \"roc_auc\",\n",
    "    }\n",
    "\n",
    "    print(f\"> Running model {name}...\")\n",
    "    results = cross_validate(estimator, X, y, cv=StratifiedKFold(shuffle=True), scoring=scoring)\n",
    "    duration = np.max(results[\"fit_time\"])\n",
    "    print(f\"  Done in {duration}s\")\n",
    "\n",
    "    # Compute auroc score\n",
    "    # See https://elitedatascience.com/imbalanced-classes (#3 Change Your Performance Metric)\n",
    "    # See https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it\n",
    "#     prob_y = estimator.predict_proba(X_test)\n",
    "#     prob_y = [p[1] for p in prob_y]\n",
    "#     auroc = roc_auc_score(y_test, prob_y)\n",
    "\n",
    "    # Create a list to store the future objects\n",
    "    futures = []\n",
    "    f1_secret = []\n",
    "    roc_auc_secret = []\n",
    "    secret_tp = []\n",
    "    secret_fp = []\n",
    "    secret_tn = []\n",
    "    secret_fn = []\n",
    "\n",
    "    # Submit the function for execution in parallel for each iteration\n",
    "    print(f\"  Evaluating classifier on test dataset...\")\n",
    "    for i in range(10):\n",
    "        f1, auroc, tp, fp, tn, fn = predict_secret(estimator, X, y, X2, y2, i)\n",
    "        f1_secret.append(f1)\n",
    "        roc_auc_secret.append(auroc)\n",
    "        secret_tp.append(tp)\n",
    "        secret_fp.append(fp)\n",
    "        secret_tn.append(tn)\n",
    "        secret_fn.append(fn)\n",
    "\n",
    "    return name, results, f1_secret, roc_auc_secret, secret_tp, secret_fp, secret_tn, secret_fn\n",
    "\n",
    "def benchmark_classifiers(X, y, competitors):\n",
    "    \"\"\"\n",
    "    Benchmarks the given classifiers by training them on the same train and test data sets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Container for the table rows\n",
    "    futures_ = []\n",
    "    data = []\n",
    "\n",
    "    # Submit the function for execution in parallel for each iteration\n",
    "    for competitor in competitors:\n",
    "        name, estimator = competitor\n",
    "        name, results, f1_secret, roc_auc_secret, secret_tp, secret_fp, secret_tn, secret_fn = benchmark_classifier(name, estimator, X, y, X2, y2)\n",
    "\n",
    "#         future = executor.submit(benchmark_classifier, name, estimator, X, y, X2, y2)\n",
    "#         futures_.append(future)\n",
    "\n",
    "#     # Iterate over the completed futures to get the results\n",
    "#     for future in concurrent.futures.as_completed(futures_):\n",
    "#         name, results, f1_secret = future.result()\n",
    "\n",
    "        data.append([\n",
    "            name,\n",
    "            np.max(results[\"fit_time\"]),\n",
    "            np.mean(results[\"test_accuracy\"]),\n",
    "            np.mean(results[\"test_tp\"]),\n",
    "            np.mean(results[\"test_fp\"]),\n",
    "            np.mean(results[\"test_tn\"]),\n",
    "            np.mean(results[\"test_fn\"]),\n",
    "            np.mean(results[\"test_f1\"]),\n",
    "            np.std(results[\"test_f1\"]),\n",
    "            np.mean(results[\"test_roc_auc\"]),\n",
    "            np.std(results[\"test_roc_auc\"]),\n",
    "#             f1,\n",
    "#             0,\n",
    "#             np.mean(results[\"test_f1_2\"]),\n",
    "#             np.std(results[\"test_f1_2\"]),\n",
    "            np.mean(f1_secret),\n",
    "            np.std(f1_secret),\n",
    "            np.mean(roc_auc_secret),\n",
    "            np.std(roc_auc_secret),\n",
    "            np.mean(secret_tp),\n",
    "            np.mean(secret_fp),\n",
    "            np.mean(secret_tn),\n",
    "            np.mean(secret_fn),\n",
    "        ])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Import models from sklearn\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFClassifier\n",
    "\n",
    "# Run all models\n",
    "rows = benchmark_classifiers(X, y, [\n",
    "    (\"GradientBoosting\", GradientBoostingClassifier(random_state=random_state)),\n",
    "    (\"HistGradientBoosting\", HistGradientBoostingClassifier(random_state=random_state)),\n",
    "    (\"DecisionTree\", DecisionTreeClassifier(random_state=random_state)),\n",
    "    (\"AdaBoost\", AdaBoostClassifier(random_state=random_state)),\n",
    "    (\"ExtraTrees\", ExtraTreesClassifier(random_state=random_state)),\n",
    "    (\"RandomForest\", RandomForestClassifier(random_state=random_state)),\n",
    "    (\"xgbDART\", XGBClassifier(random_state=random_state, booster=\"dart\")),\n",
    "    (\"xgbTree\", XGBClassifier(random_state=random_state)),\n",
    "    (\"Dummy\", DummyClassifier(random_state=random_state, strategy=\"most_frequent\")),\n",
    "])\n",
    "\n",
    "# Print results table\n",
    "table = pd.DataFrame(rows, columns=[\n",
    "    \"Algorithm\",\n",
    "    \"Time\",\n",
    "    \"Accuracy\",\n",
    "    \"True Pos\",\n",
    "    \"False Pos\",\n",
    "    \"True Neg\",\n",
    "    \"False Neg\",\n",
    "    \"F1\",\n",
    "    \"F1 SD\",\n",
    "    \"AUROC\",\n",
    "    \"AUROC SD\",\n",
    "    \"F1 (secret data)\",\n",
    "    \"F1 SD (secret data)\",\n",
    "    \"AUROC (secret data)\",\n",
    "    \"AUROC SD (secret data)\",\n",
    "    \"TP (secret)\",\n",
    "    \"FP (secret)\",\n",
    "    \"TN (secret)\",\n",
    "    \"FN (secret)\",\n",
    "])\n",
    "tlib.print_df(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2042fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_csv(\"fin_01_model_evaluation_random_state=42,more.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191ccd91",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fin_01_model_evaluation_random_state=None,more.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtlib\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfin_01_model_evaluation_random_state=None,more.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUROC\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 (secret data)\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fin_01_model_evaluation_random_state=None,more.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tlib\n",
    "\n",
    "df = pd.read_csv(\"fin_01_model_evaluation_random_state=None,more.csv\")\n",
    "\n",
    "df[\"score\"] = 0.5 * (df[\"F1\"] + df[\"AUROC\"])\n",
    "df[\"score_f1\"] = 0.5 * (df[\"F1\"] + df[\"F1 (secret data)\"])\n",
    "df[\"score_public\"] = 0.5 * (df[\"F1 (secret data)\"] + df[\"AUROC (secret data)\"])\n",
    "df[\"score_total\"] = 0.25 * (df[\"F1\"] + df[\"AUROC\"] + df[\"F1 (secret data)\"] + df[\"AUROC (secret data)\"])\n",
    "\n",
    "tlib.print_df(df)\n",
    "\n",
    "df = df.drop(df.index[[9]])\n",
    "print(np.mean(df[\"F1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7ce72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tlib\n",
    "\n",
    "X, y = tlib.load_tfw1()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8a4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27914/27914 [00:17<00:00, 1583.03it/s]\n",
      "0.957787481804949\n",
      "0.018101179100710345\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.estimators.classification import SklearnClassifier\n",
    "\n",
    "# Step 1: Create or load your pre-trained GradientBoostingClassifier model\n",
    "# For demonstration purposes, let's create a random GradientBoostingClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "def generate_adversarial_examples(model, X, y, epsilon=0.01):\n",
    "    from tqdm import trange\n",
    "    import sys\n",
    "    \n",
    "    perturbed_X = np.copy(X)\n",
    "    \n",
    "    # Iterate over each sample\n",
    "    for i in trange(len(X), file=sys.stdout):\n",
    "        # Get the original prediction\n",
    "        original_prediction = model.predict([X[i]])\n",
    "        \n",
    "        # Get the feature indices sorted by their importance\n",
    "        feature_indices = np.argsort(model.feature_importances_)[::-1]\n",
    "        \n",
    "        # Iterate over each feature and perturb it\n",
    "        for feature_index in feature_indices:\n",
    "            perturbed_X[i][feature_index] += epsilon\n",
    "            \n",
    "            # Check if the perturbed sample is misclassified\n",
    "            if model.predict([perturbed_X[i]]) != original_prediction:\n",
    "                break  # Stop perturbing the features\n",
    "            \n",
    "            # If not misclassified, reset the perturbed feature\n",
    "            perturbed_X[i][feature_index] -= epsilon\n",
    "    \n",
    "    return perturbed_X\n",
    "\n",
    "# Example usage\n",
    "# Assume you already have a trained GradientBoostingClassifier model\n",
    "# X is the input features, and y is the corresponding labels\n",
    "\n",
    "# Generate adversarial examples\n",
    "adversarial_X = generate_adversarial_examples(model, X_test.to_numpy(), y_test.to_numpy(), epsilon=0.04)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_test = f1_score(y_test.to_numpy(), model.predict(X_test.to_numpy()))\n",
    "f1_adv = f1_score(y_test.to_numpy(), model.predict(adversarial_X))\n",
    "\n",
    "# Print the predicted labels for the adversarial examples\n",
    "print(f1_test)\n",
    "print(f1_adv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
